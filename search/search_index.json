{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Course website","text":""},{"location":"#learning-outcomes","title":"Learning outcomes","text":""},{"location":"#general-learning-outcomes","title":"General learning outcomes","text":"<p>It will also cover main steps and available tools for the bioinformatics analysis of bulk ATAC-seq data, including:</p> <p>This course will provide an introduction into ATAC-seq technology and its main applications to answer epigenetics related questions.</p> <ul> <li>Recap of the steps involved in processing and aligning the raw reads</li> <li>Quality control assessment specific to ATAC-seq</li> <li>Peak calling, annotation and visualisation</li> <li>Differential accessibility (DA) analysis</li> <li>Functional enrichment analysis of DA peaks</li> </ul> <p>Theoretical and practical sessions will be combined to provide broad context on ATAC-seq analysis methods as well as hands on experience using specific tools.</p>"},{"location":"#learning-experiences","title":"Learning experiences","text":"<p>To reach the learning outcomes we will use lectures and exercises. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only.</p>"},{"location":"course_schedule/","title":"Course schedule","text":""},{"location":"course_schedule/#day-1","title":"Day 1","text":"start end subject 9:00 10:30 Introduction to ATACseq 10:30 10:45 BREAK 10:45 12:00 Filtering and QC 12:00 13:00 LUNCH BREAK 13:00 13:45 Filtering and QC (exercises) 13:45 15:00 Peak Calling 15:00 15:15 BREAK 15:15 15:45 Peak Calling (exercises) 15:45 17:00 Visualisation"},{"location":"course_schedule/#day-2","title":"Day 2","text":"start end subject 9:00 9:15 Recap of yesterday 09:15 10:30 Peaks Coverage (exercises) 10:30 10:45 BREAK 10:45 11:30 Differential Accessibility 11:30 12:00 Differential Accessibility (exercises)  12:00 13:00 LUNCH BREAK 13:30 15:00 Peaks Annotation  15:00 15:15 BREAK 15:15 17:00 Peaks Annotation (exercises)"},{"location":"exercises/","title":"Exercises","text":""},{"location":"exercises/#material","title":"Material","text":"<p> Download the presentation</p> <ul> <li>Mkdocs Website</li> <li>Mkdocs material website</li> <li>Course website template on github</li> </ul>"},{"location":"exercises/#forking-and-cloning-the-template","title":"Forking and cloning the template","text":"<p>Go to https://github.com/sib-swiss/course_website_template, and click on Use this template:</p> <p>Choose the namespace in which you want to use the website template, choose a name, and initiate the new repository by finalising with Create repository from template:</p> <p>Now you can find the new repository at <code>https://github.com/[NAMESPACE]/[REPONAME]</code>. In order to clone the repository to a local directory, click on Code and copy the github address that you can use for cloning to your clipboard:</p> <p>After that, you open a terminal (e.g. Windows Powershell or your favourite terminal) <code>cd</code> to a directory you want to clone your repository in (e.g. to <code>C:\\Users\\myname\\Documents</code>) and type:</p> <pre><code>git clone https://github.com/[NAMESPACE]/[REPONAME].git # the last part can be pasted from github\n</code></pre>"},{"location":"exercises/#serving-a-website-locally","title":"Serving a website locally","text":"<p>In order to work on your website, it is convenient if you can serve it locally and directly see the effects of your work. In order to do that use the terminal to go into the repository directory, (e.g. <code>C:\\Users\\myname\\Documents\\reponame</code>) and type:</p> <pre><code>mkdocs serve\n</code></pre> <p>Now type <code>http://localhost:8000</code> in your favourite browser, and your website should be visible.</p> <p>Note</p> <p>The website already contains content. Of course, it is up to you whether you want to keep it. In any way, you can use it as an example on how to use markdown. </p> <p>Open the file <code>index.md</code> from the directory <code>docs</code> in your favourite text editor. Add some text to the page (e.g. <code>hello world!</code>) and save the file. See whether your changes are passed to the locally served website.</p> <p>Stopping <code>mkdocs serve</code></p> <p>After you have finished working on your website you will have to stop the serving process. Otherwise, it will continue in the background and keep port 8000 (and CPU) occupied. Stop the serving process with Ctrl+C .</p>"},{"location":"exercises/#the-file-structure","title":"The file structure","text":"<p>The total file structure of the template looks like this:</p> <pre><code>.\n\u251c\u2500\u2500 LICENCE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 assets\n\u2502   \u2502   \u2514\u2500\u2500 images\n\u2502   \u2502       \u251c\u2500\u2500 SIB_logo.svg\n\u2502   \u2502       \u251c\u2500\u2500 reactions_zoom.png\n\u2502   \u2502       \u251c\u2500\u2500 reply_in_thread.png\n\u2502   \u2502       \u2514\u2500\u2500 zoom_icons.png\n\u2502   \u251c\u2500\u2500 course_schedule.md\n\u2502   \u251c\u2500\u2500 exercises.md\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 precourse.md\n\u2502   \u2514\u2500\u2500 stylesheets\n\u2502       \u2514\u2500\u2500 extra.css\n\u2514\u2500\u2500 mkdocs.yml\n\n4 directories, 12 files\n</code></pre> <p>The main directory contains:</p> <ul> <li><code>LICENCE</code>: a licence file (in this case cc-by-4.0)</li> <li><code>README.md</code>: the readme displayed at the github repository</li> <li><code>docs</code>: a directory with all website content, including:<ul> <li><code>assets</code>: a directory everything that is not directly rendered (e.g. images, pdfs)</li> <li>files ending with <code>*.md</code>: the actual markdown files that are rendered into html</li> <li><code>stylesheets</code>: a directory with <code>.css</code> file(s) for defining the website style format</li> </ul> </li> <li><code>mkdocs.yml</code>: a YAML file that is used by <code>mkdocs</code> in which you specify:<ul> <li>Website structure</li> <li>Meta information</li> <li>Plugins</li> </ul> </li> </ul>"},{"location":"exercises/#setting-up-the-website-infrastructure","title":"Setting up the website infrastructure","text":"<p>Open <code>mkdocs.yml</code> in your favourite text editor. Have a look at the first part:</p> <pre><code>site_name: Course template\n\nnav:\n    - Home: index.md\n    - Precourse preparations: precourse.md\n    - Course schedule: course_schedule.md\n    - Exercises: exercises.md\n</code></pre> <p>The first line (<code>site_name</code>) let\u2019s you change website name. Change it to something that makes sense to you, and check whether it has changed in the locally hosted site.</p> <p>With the part named <code>nav</code>, you can change the website structure and with that navigation. The file <code>index.md</code> should always be there, this is the \u2018homepage\u2019.</p> <p>Now we will generate a new page that is a subchapter of Exercises. In order to do so, follow the following steps:</p> <ul> <li>Generate a directory within the directory <code>docs</code> called <code>exercises</code></li> <li>Within the <code>exercises</code> directory generate a new file called <code>exercises_day1.md</code></li> <li>Adjust the <code>nav</code> part of <code>mkdocs.yml</code> like so:</li> </ul> <pre><code>nav:\n    - Home: index.md\n    - Precourse preparations: precourse.md\n    - Course schedule: course_schedule.md\n    - Exercises:\n      - Day 1: exercises/exercises_day1.md\n</code></pre> <p>Now a new collapsible menu will appear, containing your new page.</p>"},{"location":"exercises/#referring-to-the-right-repo","title":"Referring to the right repo","text":"<p>In <code>mkdocs.yml</code> have a look at the repository part:</p> <pre><code># Repository\nrepo_name: sib-swiss/course_website_template\nrepo_url: https://github.com/sib-swiss/course_website_template\n</code></pre> <p>The course website is now hosted at your own repository. Therefore, change the repository name and url according to your own.</p>"},{"location":"exercises/#markdown-syntax","title":"Markdown syntax","text":"<p>You can use general github markdown syntax in order to generate a formatted html page. Have a look here.</p> <p>Now, convert the rendered text below into markdown. Add your markdown text to the file <code>exercises_day1.md</code> and see whether you get the expected result while you type.</p> Rendered markdown Answer <pre><code>### My markdown exercise\n\nWith plain markdown you can highlight in two ways:\n\n1. *Italic*\n2. **Bold**\n\nYou can add a link to your favourite [website](https://www.sib.swiss/).\nOr add an image from that website (find it at `https://www.sib.swiss/images/banners/banner_research_infrastructure.jpg`):\n\n![](https://www.sib.swiss/images/banners/banner_research_infrastructure.jpg)\n\nYou can also add a local image (this one is stored in `../assets/images/zoom_icons.png`):\n\n![](../assets/images/zoom_icons.png)\n\nSharing a code is easy, inline you refer to code like this: `pip install mkdocs`.\nBut often it's more convenient in a code block, e.g. with shell highlighting:\n\n```sh\nFILE=my_genes.csv\ncat $FILE | cut -f 1,2 -d ','\n```\n\nOr with R highlighting for example:\n\n```r\ndf &lt;- read.csv('my_genes.csv')\n```\n</code></pre>"},{"location":"exercises/#my-markdown-exercise","title":"My markdown exercise","text":"<p>With plain markdown you can highlight in two ways:</p> <ol> <li>Italic</li> <li>Bold</li> </ol> <p>You can add a link to your favourite website. Or add an image from that website (find it at <code>https://www.sib.swiss/images/banners/banner_research_infrastructure.jpg</code>):</p> <p></p> <p>You can also add a local image (this one is stored in <code>../assets/images/zoom_icons.png</code>):</p> <p></p> <p>Sharing a code is easy, inline you refer to code like this: <code>pip install mkdocs</code>. But often it\u2019s more convenient in a code block, e.g. with shell highlighting:</p> <pre><code>FILE=my_genes.csv\ncat $FILE | cut -f 1,2 -d ','\n</code></pre> <p>Or with R highlighting for example:</p> <pre><code>df &lt;- read.csv('my_genes.csv')\n</code></pre>"},{"location":"exercises/#additional-features-of-mkdocs-material","title":"Additional features of Mkdocs material","text":"<p>Some additional features are very convenient for generating a website for teaching. For example admonitions:</p> code <pre><code>!!! warning\n    Do not overcommit the server!\n</code></pre> output <p>Warning</p> <p>Do not overcommit the server!</p> <p>Also very convenient can be content tabs:</p> <p>code:</p> <pre><code>=== \"R\"\n    Generating a vector of integers:\n    ```r\n    a &lt;- c(5,4,3,2,1)\n    ```\n=== \"python\"\n    Generating a list of integers:\n    ```python\n    a = [5,4,3,2,1]\n    ```\n</code></pre> <p>output:</p> R <p>Generating a vector of integers: <pre><code>a &lt;- c(5,4,3,2,1)\n</code></pre></p> python <p>Generating a list of integers: <pre><code>a = [5,4,3,2,1]\n</code></pre></p> <p>Mkdocs material comes with a very wide range of emoticons and icons. Use the search field in the link to search for icons. Here\u2019s an example:</p> code <pre><code>Write an e-mail :material-send:, add a pdf :material-file-pdf: and wait :clock1:\n</code></pre> output <p>Write an e-mail , add a pdf :material-file-pdf: and wait </p> <p>You can make a button like this:</p> code <pre><code>[Download the presentation](../assets/pdf/introduction_gh_pages.pdf){: .md-button }\n</code></pre> output <p>Download the presentation</p> <p>You can also add an icon to a button:</p> code <pre><code>[:fontawesome-solid-file-pdf: Download the presentation](../assets/pdf/introduction_gh_pages.pdf){: .md-button }\n</code></pre> output <p> Download the presentation</p> <p>Lastly, you can incorporate <code>html</code>. This can particularly be convenient if you want to control the size of images.</p> code <pre><code>&lt;figure&gt;\n  &lt;img src=\"../assets/images/zoom_icons.png\" width=\"300\"/&gt;\n&lt;/figure&gt;\n\n&lt;figure&gt;\n  &lt;img src=\"../assets/images/zoom_icons.png\" width=\"100\"/&gt;\n&lt;/figure&gt;\n</code></pre> output <p> </p> <p> </p>"},{"location":"exercises/#byo-workshop","title":"BYO workshop","text":"<p>If you have brought your own course material, now you can start with generating a page containing your own course material.</p>"},{"location":"exercises/#host-the-website-at-githubio","title":"Host the website at github.io","text":"<p>You can deploy your website as a github page by running the command:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>It will become available at <code>[NAMESPACE].github.io/[REPONAME]</code>. This can take more than an hour if you are deploying for the first time. The next time you update your website, it will usually take less then a minute.</p>"},{"location":"exercises/#pushing-to-a-remote-repository","title":"Pushing to a remote repository","text":"<p>The website html is created in a directory called <code>site</code> inside your repository directory. This directory is used to locally host the website, but usually you don\u2019t want to push it to your master branch. Therefore add it to <code>.gitignore</code>:</p> <pre><code>echo \"site\" &gt;&gt; .gitignore\n</code></pre> <p>Note</p> <p>Anything that is added to the file <code>.gitignore</code> is not added to the git repository. You have to add such files/directories only once. You can of course also open <code>.gitignore</code> in your favourite text editor and modify it in there.</p> <p>Now, you can add your changes to make a commit:</p> <pre><code>git add --all\n</code></pre> <p>And commit your changes to your local repository like this:</p> <pre><code>git commit -m 'short description'\n</code></pre> <p>And finally push it to the remote:</p> <pre><code>git push\n</code></pre>"},{"location":"precourse/","title":"Precourse preparations","text":""},{"location":"precourse/#knowladge","title":"Knowladge","text":""},{"location":"precourse/#ngs","title":"NGS","text":"<p>As announced in the course registration webpage, we expect participants to already have a basic knowledge in Next Generation Sequencing (NGS) techniques.</p>"},{"location":"precourse/#unix","title":"UNIX","text":"<p>Practical knowledge of the UNIX command line is also required to be able to follow this course.</p>"},{"location":"precourse/#r","title":"R","text":"<p>A basic knowledge of the R language is required to perform most analytical stepsn the downstream analysis.</p>"},{"location":"precourse/#technical","title":"Technical","text":"<p>Attendees should have a Wi-Fi enabled computer and the Integrative Genomics Viewer (IGV) installed.</p> <p>An online R and RStudio environment will be provided. In order to access that environment your computer needs to be able to access http websites (not https). You can check this by browsing this website.</p>"},{"location":"days/01_filter_bam_files/","title":"01 Filter bam files","text":"<p>Post Alignment reads filtering</p>"},{"location":"days/01_filter_bam_files/#0-dataset","title":"0. Dataset","text":"<p>We are going to work with a subset of the publicly available dataset from Galang.et al. 2020  (GEO accession)</p> <p>Out dataset contains 2 conditions, with 2 replicates each: PC: Rep1, Rep2 RACM: Rep1, Rep2 </p> <p>If you are interested in the biological questions behind, have a look to the publication.</p> <p>In order to save time, the raw reads have already been trimmed using Trim Galore and mapped to the reference genome (mm10) using bowtie2 in end-to-end mode.  </p> <p>We will start the analysis directly from the alignment output (.bam files). To avoid large waiting times during high-demanding computational steps .bam files have been subset to keep only reads aligning to chromosome 13.  </p> <p>Note</p> <p>You can find the .bam files in <code>/data/input_data/alignments_Galang_chr13/</code> folder</p>"},{"location":"days/01_filter_bam_files/#1-filtering-reads-from-alignments","title":"1. Filtering reads from alignments","text":""},{"location":"days/01_filter_bam_files/#general-ngs-filtering-steps","title":"General NGS filtering steps","text":"<p>In order to keep good quality data, read filtering criteria can be applied. These creteria are similar to the ones we would apply when analysing any other NGS related dataset (ie. remove duplicated reads, read mapping quality thresholds, etc.). They will depend on the quality of the data and the downstream analysis that will be applied.</p> <p>We can use tools like picard to indetify and mark read duplicates, which originate most likely from PCR amplifications. This step has already been performed for you, and read duplicates have been marked in the provided .bam files. If you want to know more about this tool and how it works you can have a look here.</p> <p>Task 1: </p> <ul> <li> <p>Create an output folder named: <code>results/01_filtered_bams</code></p> </li> <li> <p>Using <code>samtools view</code>, select/filter reads from .bam files in order to apply the following criteria: </p> <ul> <li>Keep only paired reads </li> <li>Remove unmapped reads</li> <li>Remove reads who\u2019s mate is unmapped</li> <li>Remove read duplicates</li> <li>Remove not primary alignment reads</li> <li>Keep mapping quality &gt; 10</li> </ul> </li> </ul> <p>Save the results in <code>results/01_filtered_bams</code> in BAM format with the following output name: <code>&lt;sample name&gt;.qc_filt.bam</code></p> <p>Samtools</p> <p>Use the samtools view manual here to understand which parameters you need, and this useful page from the broad institue to find what SAM Flag value would correspond to the combination of criteria we want to apply. </p> Hint You can use `-f` and `-F` flags to filter reads based on a combination of SAM Flags. You can use -q for MapQ, and -h and -b for output format.   Solution <pre><code># create new directory for filtered bams\n\nmkdir -p results/01_filtered_bams\n\n# filter files using a for loop\n\nfor bam in data/alignments_Galang_chr13/*.bam; do\n    echo \"Processing file: $bam\"\n    sample_name=$(basename \"$bam\" .bam) # extract sample name without path and extension\n    samtools view -h -b -q 10 -f 1 -F 1292 --output results/01_filtered_bams/$sample_name.qc_filt.bam $bam\ndone\n</code></pre> <p>-h: keep header; -b: output in bam format; -q 1: minimum mapping quality 10; -f 1: keep paired; -F 1292: exclude reads with any of the following flags: read unmapped, mate unmapped, not primary alignment, read is duplicate</p> <p>Once we have our filtered bams, we will sort them and index them</p> <p>Task 2: </p> <ul> <li>Use <code>samtools sort</code> to sort the filtered bam files (from previous task: <code>&lt;sample name&gt;.qc_filt.bam</code>). Save them in the same folder as: <code>&lt;sample name&gt;.qc_filt.sorted.bam</code></li> <li>Use <code>samtools index</code> to index the sorted bam files </li> </ul> Solution <pre><code>for bam in results/01_filtered_bams/*qc_filt.bam; do\n    echo \"Processing file: $bam\"\n    samtools sort -o ${bam%.bam}.sorted.bam $bam\n    samtools index ${bam%.bam}.sorted.bam\ndone\n</code></pre>"},{"location":"days/01_filter_bam_files/#atacseq-related-filtering-steps","title":"ATACseq related filtering steps","text":"<p>Mitochondria DNA is nucleosome free, therefore it is more accessible for Tn5 and several reads may have originated from mitochondrial DNA. After having assessed mitochondrial % on the QC, we can discard reads coming from chmMT to avoid biases in downstream analsis.</p> <p>Since we are working only with chm 13 we don\u2019t need to do this step, but here is the command you could use for that: <pre><code>    samtools view -h input.bam | awk  '($3 != \"MT\")' | samtools view -hb - &gt; output.bam\n</code></pre></p> <p>Next, we will remove reads overlapping problematic regions of the genome. ENCODE consortium has created comprehensive lists of such regions (anomalous, unstructured or high signal in NGS experiments) for different genome species (including mouse mm10). These lists are called ENCODE Blacklists, and you can find them here. </p> <p>Note</p> <p>The regions for mm10 have been dowloaded as .bed file, and you will find it here: <code>data/references/mm10-blacklist.v2.nochr.bed</code></p> <p>Task 3</p> <ul> <li>Using <code>bedtools intersect</code> filter out reads overlapping regions in the Blacklist .bed file</li> <li>Use previously filtered .bam files as input: <code>results/01_filtered_bams/*qc_filt.sorted.bam</code>, don\u2019t forget to specify your input is in BAM format</li> <li>Use the <code>data/references/mm10-blacklist.v2.nochr.bed</code> as regions to filter out reads from (it is already sorted)</li> <li>Save the results in <code>results/01_filtered_bams</code> in BAM format with the following output name: <code>&lt;sample name&gt;.qc_bl_filt.bam</code> </li> <li>Sort and index the resulting .bam with <code>samtools</code> and save them in <code>results/01_filtered_bams</code> with name  <code>&lt;sample name&gt;.qc_bl_filt.sorted.bam</code> </li> </ul> <p>Bedtools</p> <p>Bedtools intersect allows one to screen for overlaps between two sets of genomic features/regions, and then decide on which kind of information do you want to report. Here, we will intersect: a) aligned reads to the genome (information in your .bam files)l b) Problematic regions listed in Blacklist .bed file We do not want to keep the reads that overlap Blacklist regions. You can find documentation on which parameters to use here</p> Solution <pre><code>blacklist=\"data/references/mm10-blacklist.v2.nochr.bed\"\n\nfor bam in results/01_filtered_bams/*qc_filt.sorted.bam; do\n    echo \"Processing file: $bam\"\n    sample_name=$(basename \"$bam\" .qc_filt.sorted.bam) \n    bedtools intersect -v -abam $bam -b $blacklist &gt; results/01_filtered_bams/$sample_name.qc_bl_filt.bam\n    samtools sort -o results/01_filtered_bams/$sample_name.qc_bl_filt.sorted.bam results/01_filtered_bams/$sample_name.qc_bl_filt.bam\n    samtools index results/01_filtered_bams/$sample_name.qc_bl_filt.sorted.bam\ndone\n</code></pre> <p>Parameter explanation:  -v: only report those entries in A that have no overlap with B  -abam: input is in bam format </p> <p>After sorting the .bam files, we don\u2019t need the unsorted .bam. To free some space we will remove the unsorted bam files (intermediate files).</p> <p>Task 4</p> <ul> <li>Remove intermediate files that we don\u2019t need anymore. </li> </ul> <p>Warning</p> <p>Run this script only if you named the files in the same way the tutorial mentioned, and only if you have finished the sorting and indexing steps from Task 2 and 3</p> <pre><code>    rm results/01_filtered_bams/*qc_filt.bam\n    rm results/01_filtered_bams/*qc_bl_filt.bam\n</code></pre>"},{"location":"days/02_QC_post_alignment/","title":"02 QC post alignment","text":"<p>In order to assess the quality of the ATACseq experiment, there are several metrics that can be calculated from the aligned reads.</p> <p>Important metrics include:  - Fragment size distribution: The distribution of fragment sizes can indicate the quality of the library preparation. A good ATAC-seq library should have a characteristic pattern with peaks corresponding to nucleosome-free regions (NFRs) and mono-, di-, and tri-nucleosomes.  - TSS enrichment: The enrichment of reads at transcription start sites (TSS) is a key indicator of data quality. High-quality ATAC-seq data should show a strong enrichment of reads at TSSs.</p> <p><code>ATAQV</code> is a tool that calculates a variety of QC metrics for ATAC-seq data. It provides a comprehensive report that includes fragment size distribution, TSS enrichment, and other important metrics. </p> <p>Task 1</p> <ul> <li>Create a new folder for QC results called: <code>results/02_QC_post_aligment</code></li> <li>Run ATAQV on each filtered .bam file </li> <li>Using ATAQV tool, create a report from all ATAQV sample results</li> </ul> <p>Ataqv</p> <p>You can find further information about ATAQC tool and commands in the Usage section here Ataqv needs a Transcription Start Site (TSS) reference file to compute TSS enrichment score. You will find this file in: <code>data/references/ENCODE_mm10_M21_TSS_reference.bed</code> We have downloaded the TSS reference gile from ENCODE project (https://www.encodeproject.org/files/ENCFF498BEJ/)</p> Hint    First run ataqv for each \"*qc_bl_filt.sorted.bam\" file, important parameters are: --tss-file, mouse, --metrics-file    Second run mkarv with all \"*json\" files  Solution <p><pre><code>mkdir -p results/02_QC_post_aligment\nTSS_bed=\"data/references/ENCODE_mm10_M21_TSS_reference.bed\"\n\nfor bam in results/01_filtered_bams/*qc_bl_filt.sorted.bam; do\n    echo \"Processing file: $bam\"\n    sample_name=$(basename \"$bam\" .qc_bl_filt.sorted.bam) # extract sample name without path and extension\n    echo $sample_name\n    ataqv --name $sample_name --metrics-file results/02_QC_post_aligment/$sample_name.ataqv.json --tss-file $TSS_bed mouse $bam &gt; results/02_QC_post_aligment/$sample_name.ataqv.out\ndone\n\ncd results/02_QC_post_aligment\nmkarv summary_ataqv PC_rep1.ataqv.json PC_rep2.ataqv.json RACM_rep1.ataqv.json RACM_rep2.ataqv.json \ncd ../../\n</code></pre> parametr explanation:  \u2013name: sample name for output files  \u2013metrics-file: output file for metrics in json format  \u2013tss-file: bed file with TSS locations  mouse: genome (can be mouse or human)  $bam: input bam file mkarv is a tool to create a summary report from multiple ataqv json files</p> <p>Task 2</p> <p>Open the QC report <code>results/02_QC_post_aligment/summary_ataqv/index.html</code> and have a look at the QC metrics. Do you think the experiment worked well?  Is there some metrics that would concern you?</p>"},{"location":"days/03_peak_calling/","title":"03 Peak calling","text":""},{"location":"days/03_peak_calling/#subset-bams-to-keep-only-nf-fragments","title":"Subset .bams to keep only NF fragments","text":"<p>Before doing peak calling with MACS3, we will split fragments into nucleosome-free (NF) and nucleosome-associated fragments.  For that, we will filter the BAM files based on fragment length (insert size). We will then call the peaks based on NF reads only, and compare with peaks called on all reads (?) Finally, we will also try peak calling with HMMRATAC, which is a peak caller specifically designed for ATAC-seq data and uses a Hidden Markov Model to identify accessible chromatin regions. (?)</p> <p>Task 1</p> <ul> <li>Create an output folder named: <code>results/01_filtered_bams/NF_bams</code></li> <li>Using <code>samtool view</code> keep only fragments with length: 1 - 100 bp. Keep output format as .bam file, and save them in <code>results/01_filtered_bams/NF_bams</code>. Follow the naming: ${sample_name}_NF.bam</li> <li>While doing that, keep the header of .bam file </li> </ul> <p>Note</p> <p>In SAM format, column 9 is described as \u201cTLEN: signed observed Template LENgth\u201d, which corresponds to the insert size </p> Hint You can first use samtools view, and then use awk commands to filter the reads for those which column 9 is: ($9&gt;= 1 &amp;&amp; $9&lt;=100) || ($9&lt;=-1 &amp;&amp; $9&gt;=-100)  Solution <p><pre><code># create new directory for peaks\nmkdir -p results/01_filtered_bams/NF_bams\n\n# filter for NF fragments only\n\nfor bam in results/01_filtered_bams/*qc_bl_filt.sorted.bam; do\n    echo \"Processing sample: $sample_name\"\n    sample_name=$(basename \"$bam\" .qc_bl_filt.sorted.bam)\n    samtools view -h $bam | awk 'substr($0,1,1)==\"@\" || ($9&gt;= 1 &amp;&amp; $9&lt;=100) || ($9&lt;=-1 &amp;&amp; $9&gt;=-100)' | \\\n    samtools view -b &gt; results/01_filtered_bams/NF_bams/${sample_name}_NF.bam \ndone \n</code></pre> Parameter explanation:  -h: keep header; -b: output in bam format; $9: insert size (TLEN field in SAM format); keep fragments with insert size between 1 and 100bp</p> <p>Task 2</p> <ul> <li>Sort and index the bams files for next step</li> <li>Remove the unsorted file</li> </ul> Solution <pre><code>for bam in results/01_filtered_bams/NF_bams/*_NF.bam; do\n    sample_name=$(basename \"$bam\" _NF.bam)\n    samtools sort -o results/01_filtered_bams/NF_bams/${sample_name}_NF.sorted.bam results/01_filtered_bams/NF_bams/${sample_name}_NF.bam\n    rm results/01_filtered_bams/NF_bams/${sample_name}_NF.bam\n    samtools index results/01_filtered_bams/NF_bams/${sample_name}_NF.sorted.bam\ndone\n</code></pre>"},{"location":"days/03_peak_calling/#peak-calling","title":"Peak calling","text":""},{"location":"days/03_peak_calling/#macs3-based-on-nf-fragments","title":"MACS3 based on NF fragments","text":"<p>Next we will call peaks using MACS3 on the NF reads only. This way, we focus on the fragments that are most likely to represent open chromatin regions only (nucleosome-free regions).</p> <p>MACS3 is a widely used peak calling tool that can handle both narrow and broad peaks. We will start using the \u201ccallpeak\u201d function of MACS3 to identify peaks in the NF reads.</p> <p>Task 3</p> <ul> <li>Create a new folder named: <code>results/03_peak_calling</code></li> <li>Create a subfolder inside called: <code>NF_peaks</code></li> <li>Call peaks on NF reads using MACS3 callpeak function. Save the results inside r<code>esults/03_peak_calling/NF_peaks/</code> and name the files as: <code>NF_peaks_${sample_name}</code></li> </ul> <p>Macs3</p> <p>You can have a look at the MACS3 documentation for more details on the parameters used here</p> Hint     For ATAC-seq data, we will use the BAMPE format, which is suitable for paired-end data and we will set the genome size to \"mm\" for mouse. We will also set a q-value cutoff of 0.01 to control the false discovery rate.  Solution <p><pre><code>mkdir -p results/03_peak_calling/NF_peaks\n\nsamples=(PC_rep1 PC_rep2 RACM_rep1 RACM_rep2)\n\n# process NF reads\nbams_path=\"results/01_filtered_bams/NF_bams\"\n\nfor sample_name in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample\"\n    macs3 callpeak -f BAMPE -t $bams_path/${sample_name}_NF.sorted.bam -g mm -q 0.01 --outdir results/03_peak_calling/NF_peaks/NF_peaks_${sample_name}/\ndone    \n</code></pre> parameters explanation: -f BAMPE: input file format is BAM paired-end -t: input file (BAM file) -g mm: genome size (mm for mouse, hs for human) -q 0.01: q-value cutoff for peak detection \u2013outdir: output directory for peak files  </p> <p>We will do the same, but using all fragments in filtered .bam files </p> <p>Task 4</p> <ul> <li>Create a subfolder inside called: <code>all_peaks</code></li> <li>Call peaks on all filtered reads using MACS3 callpeak function. Save the results inside r<code>esults/03_peak_calling/all_peaks/</code> and name the files as: <code>all_peaks_${sample_name}</code></li> </ul> Solution <pre><code>bams_path=\"results/01_filtered_bams/\"\nmkdir -p results/03_peak_calling/all_peaks\n\nfor sample in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample\"\n    macs3 callpeak -f BAMPE -t $bams_path/${sample}.qc_bl_filt.sorted.bam -g mm -q 0.01 --outdir results/03_peak_calling/all_peaks/all_peaks_${sample}/\ndone\n</code></pre>"},{"location":"days/04_build_consensus_peak_set/","title":"04 Build consensus peak set","text":""},{"location":"days/04_build_consensus_peak_set/#visualisation-of-peaks","title":"Visualisation of peaks","text":"<p>We will start by having a look into the previous results. For that, we can use IGV</p>"},{"location":"days/04_build_consensus_peak_set/#buid-a-consensus-peaks-annotation","title":"Buid a consensus peaks annotation","text":"<p>We need to build a consensus peak set for downstream analysis (e.g. differential accessibility analysis), which will become like our reference annotation for counting reads in peaks. We will do this by intersecting the peaks called in each replicate for each condition, and then merging the peaks from both conditions into a final consensus peak set.</p> <p>Run the following commands to create intersected peak files for each condition (PC and RACM), requiring at least 25% overlap (-f 0.25) and reciprocal overlap (-r)</p> <p>Code</p> <p>Create a folder for consensus peaks. Use bedtools intersect to keep peaks present in both replicates</p> <p><pre><code>    mkdir results/04_consensus_peaks\n    path_peaks=\"results/03_peak_calling/all_peaks\"\n\n    bedtools intersect -wa -a $path_peaks/all_peaks_PC_rep1/NA_peaks.narrowPeak -b $path_peaks/all_peaks_PC_rep2/NA_peaks.narrowPeak -f 0.25 -r &gt; results/04_consensus_peaks/PC_intersect.bed\n    bedtools intersect -wa -b $path_peaks/all_peaks_PC_rep1/NA_peaks.narrowPeak -a $path_peaks/all_peaks_PC_rep2/NA_peaks.narrowPeak -f 0.25 -r &gt;&gt; results/04_consensus_peaks/PC_intersect.bed\n    bedtools intersect -wa -a $path_peaks/all_peaks_RACM_rep1/NA_peaks.narrowPeak -b $path_peaks/all_peaks_RACM_rep2/NA_peaks.narrowPeak -f 0.25 -r &gt; results/04_consensus_peaks/RACM_intersect.bed\n    bedtools intersect -wa -b $path_peaks/all_peaks_RACM_rep1/NA_peaks.narrowPeak -a $path_peaks/all_peaks_RACM_rep2/NA_peaks.narrowPeak -f 0.25 -r &gt;&gt; results/04_consensus_peaks/RACM_intersect.bed\n</code></pre> Parameters explanation: -a: first input file (bed or narrowPeak format) -b: second input file (bed or narrowPeak format) -f 0.25: minimum overlap required as a fraction of A -r: require reciprocal overlap -wa: write the original entry in A for each overlap  </p> <p>To get all the columns from both files, we need to run bedtools intersect twice, swapping -a and -b</p> <p>Code</p> <p>Merge the intersected peaks from both conditions into a final consensus peak set, allowing a maximum distance of 10bp between peaks to be merged (-d 10)</p> <pre><code>cat results/04_consensus_peaks/PC_intersect.bed results/04_consensus_peaks/RACM_intersect.bed | sort -k1,1 -k2,2n | bedtools merge -d 10 -i - &gt; results/04_consensus_peaks/consensus_peaks.bed\n</code></pre> <p>Add the <code>results/04_consensus_peaks/consensus_peaks.bed</code> track to IGV and have a look to the resulting peak annotation. </p>"},{"location":"days/05_quantify_peaks/","title":"05 Quantify peaks","text":"<p>After defining a consensus peak set, we will count the number of reads that overlap each peak in each sample. This step can be seen as counting reads in annotated genes for RNA-seq data, where instead of genes, we are counting reads in peaks.</p> <p>There are different tools that can be used for this purpose, such as featureCounts from the Subread package or bedtools coverage. In this example, we will use featureCounts, which is a widely used tool for counting reads in genomic features. </p> <p>Task 1</p> <p>We first need to convert the consensus peaks to SAF format, which is the format required by featureCounts The SAF format is a tab-delimited file with the following columns: GeneID, Chr, Start, End, Strand</p> <ul> <li>Create a new folder named: <code>results/05_counts_reads</code></li> <li>Use awk or an alternative way to convert the bed file to SAF format. Don\u2019t forget to add a header \u201cGeneID    Chr Start   End Strand\u201d. </li> </ul> Solution <pre><code>mkdir results/05_counts_reads\necho \"GeneID    Chr Start   End Strand\" &gt; results/04_consensus_peaks/consensus_peaks.saf\nawk '{OFS = \"\\t\"} {print \"Interval_\"NR,$1,$2,$3,\".\"}' results/04_consensus_peaks/consensus_peaks.bed &gt;&gt; results/04_consensus_peaks/consensus_peaks.saf\n</code></pre> <p>Now we can use featureCounts to count the reads in each peak for each sample</p> <p>Task 2</p> <ul> <li>Run featureCounts to count filtered reads (from \u201c*qc_bl_filt.sorted.bam\u201d) into consensus peak annotation</li> </ul> <p>Featurecounts</p> <p>-F SAF: specify that the annotation file is in SAF format -p: specify that the input files are paired-end -a: specify the annotation file -o: specify the output file  </p> <p>These are only some parameteres that we will apply. You can also adjust the parameters of featureCounts based on your specific requirements, such as setting a minimum mapping quality or handling multi-mapping reads.</p> Solution <pre><code>path_bams=\"results/01_filtered_bams\"\neatureCounts -F SAF -p -a results/04_consensus_peaks/consensus_peaks.saf -o results/05_counts_reads/feature_Counts.txt $path_bams/*qc_bl_filt.sorted.bam 2&gt; results/05_counts_reads/featureCounts.log\n</code></pre> <p>The output file will contain the counts of reads in each peak for each sample, which can be used for downstream analysis such as differential accessibility analysis.</p> <p>After filtering, peak calling and counting reads on peaks, we can run MultiQC to aggregate the QC metrics from different steps and generate a comprehensive report. This will help us to assess further he overall quality of the ATAC-seq data and identify any potential issues that may need to be addressed before proceeding with downstream analysis.  </p> <p>Task 3 </p> <ul> <li>Run multiqc on the entire <code>results</code> directory</li> </ul> Solution <pre><code>multiqc --outdir results/multiQC_report --title multiQC_post_read_counting results\n</code></pre>"},{"location":"days/06_differential_accessibility_analysis/","title":"06 Differential accessibility analysis","text":"<p>In the next step, we will start importing the read counts on consensus peaks (output from featureCounts) into R and perform differential accessibility analysis using DESeq2.</p>"},{"location":"days/06_differential_accessibility_analysis/#import-read-counts-and-metadata","title":"Import read counts and metadata","text":"<p>We start by loading featureCounts output as a matrix of counts</p> <p>Load necessary packages for the analysis     <pre><code>library(DESeq2)\nlibrary(ggplot2)\nlibrary(dplyr)\n</code></pre></p> <p>Task 1</p> <ul> <li>Read the output of featurecounts that contains counts for all peaks in all samples: <code>results/05_counts_reads/feature_Counts.txt</code></li> <li>Keep the header of the table</li> <li>Process the table in order to keep: read-counts columns only, peak IDs as row.names, simple column names (sample names)</li> <li>Convert the table into matrix format</li> </ul> Solution <pre><code># Read table of counts (output from FeatureCounts)\ncounts_file &lt;- \"results/05_counts_reads/feature_Counts.txt\"\ncts_table &lt;- read.table(counts_file, header = T)\n\n# convert into matrix, with interval IDs as rownames\ncts &lt;- cts_table[,7:10]\nrow.names(cts) &lt;- cts_table[,1]\ncolnames(cts) &lt;- gsub(colnames(cts), pattern = \"results.01_filtered_bams.\", replacement = \"\") %&gt;% \n                 gsub(., pattern=\".qc_bl_filt.sorted.bam\", replacement=\"\")\n</code></pre> <p>** Task 2**</p> <p>Create metadata table We need to create a data.frame containing metadata information about the samples. Here we have two conditions, PC and RACM, with two replicates each. We need to create a data.frame containing the conditions information for each sample.  If we would have a second factor, e.g. treatment (treated vs untreated), we would need to add this information as a second column in the data.frame.</p> <p>!!! Note:</p> <pre><code>Important: The order of the rows in the data.frame must match the order of the columns in the counts matrix\n           The metadata data.frame must have factor variables (not character variables)\n</code></pre> <pre><code>condition &lt;- factor( c(rep(\"PC\",2), rep(\"RACM\",2)) )\ncolData &lt;- data.frame(condition, row.names = colnames(cts))\n</code></pre>"},{"location":"days/06_differential_accessibility_analysis/#deseq-object-and-analysis","title":"DEseq object and analysis","text":"<p>Bring together counts and metadata to create a DESeq object</p> <pre><code>dds &lt;- DESeqDataSetFromMatrix(\n  countData = cts, colData = colData, \n  design = ~ condition)\ndim(dds)\n</code></pre> <p>Remove lowly exp peaks</p> <pre><code>idx &lt;- rowSums(counts(dds, normalized=FALSE) &gt;= 50) &gt;= 2\ndds.f &lt;- dds[idx, ]\ndim(dds.f)\n</code></pre> <p>We perform the estimation of dispersions</p> <pre><code>dds &lt;- DESeq(dds.f)\n</code></pre> <p>And plot PCA of the samples <pre><code>vsd &lt;- varianceStabilizingTransformation(dds.f, blind=TRUE )\npcaData &lt;- plotPCA(vsd, intgroup=c(\"condition\"), ntop=\"all\")\npcaData + geom_label(aes(x=PC1,y=PC2,label=name))\nplotPCA(vsd, intgroup=c(\"sizeFactor\"))\n</code></pre></p>"},{"location":"days/06_differential_accessibility_analysis/#differential-accessibility-results","title":"Differential Accessibility results","text":"<p>After the dispersion estimates have been calculated, we can proceed to test for differential accessibility between the two conditions (PC vs RACM).  </p> <p>The <code>results()</code> function extracts a results table with the log2 fold changes, p-values and adjusted p-values for each peak. <pre><code>res &lt;- results(dds)\nsummary( res )\n</code></pre></p> <p>Note</p> <p>By default, the results() function will extract the results for the last variable in the design formula (here: condition) and will perform a comparison of the second level of the factor over the first level (here: RACM over PC). If you want to extract results for a different comparison, you can specify the contrast argument. Have a look at DESeq2 documentation for more information.</p> <p>We can visualise the results of DA peaks with a Volcano plot, analogous to RNAseq DE genes results</p> <pre><code>\n</code></pre>"},{"location":"days/06_differential_accessibility_analysis/#create-granges-object","title":"Create GRanges object","text":"<p>We will convert the peak coordinates into a GRanges object (from GenomicRanges package).  Granges class represents a collection of genomic ranges and associated data to them. In this case, we will use it to represent the peak coordinates, and we will add as metadata the log2 fold changes and adjusted p-values from the differential accessibility analysis.</p> <p><pre><code>    # Prepare peak coordinates object \n    peaks_coord &lt;- cts_table[,1:4]\n\n    # Select information from DESeq2 we want to keep\n    DA_stats &lt;- as.data.frame(res)[c(2,3,5,6)]\n\n    # Merge both objects and convert them into GRanges class\n    peaks_df &lt;- merge(DA_stats, peaks_coord, by.x=\"row.names\", by.y=\"Geneid\")\n    peaks_gr = makeGRangesFromDataFrame(peaks_df, keep.extra.columns=T)\n\n    # Have a look into the new object\n    head(peaks_gr)\n</code></pre> To this object <code>peaks_gr</code>, we will add other metadata along the downstream analysis. </p>"},{"location":"days/07_annotate_peaks/","title":"07 annotate peaks","text":"<p>library(ChIPseeker) library(TxDb.Mmusculus.UCSC.mm10.knownGene) library(clusterProfiler)</p> <p>peak = makeGRangesFromDataFrame(cts_table, keep.extra.columns=T) covplot(peak)</p>"}]}