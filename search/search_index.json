{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Course website","text":""},{"location":"#learning-outcomes","title":"Learning outcomes","text":""},{"location":"#general-learning-outcomes","title":"General learning outcomes","text":"<p>It will also cover main steps and available tools for the bioinformatics analysis of bulk ATAC-seq data, including:</p> <p>This course will provide an introduction into ATAC-seq technology and its main applications to answer epigenetics related questions.</p> <ul> <li>Recap of the steps involved in processing and aligning the raw reads</li> <li>Quality control assessment specific to ATAC-seq</li> <li>Peak calling, annotation and visualisation</li> <li>Differential accessibility (DA) analysis</li> <li>Functional enrichment analysis of DA peaks</li> </ul> <p>Theoretical and practical sessions will be combined to provide broad context on ATAC-seq analysis methods as well as hands on experience using specific tools.</p>"},{"location":"#learning-experiences","title":"Learning experiences","text":"<p>To reach the learning outcomes we will use lectures and exercises. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only.</p>"},{"location":"course_schedule/","title":"Course schedule","text":""},{"location":"course_schedule/#day-1","title":"Day 1","text":"start end subject 9:00 10:30 Introduction to ATACseq 10:30 10:45 BREAK 10:45 12:00 Filtering and QC 12:00 13:00 LUNCH BREAK 13:00 13:45 Filtering and QC (exercises) 13:45 15:00 Peak Calling 15:00 15:15 BREAK 15:15 15:45 Peak Calling (exercises) 15:45 17:00 Visualisation"},{"location":"course_schedule/#day-2","title":"Day 2","text":"start end subject 9:00 9:15 Recap of yesterday 09:15 10:30 Peaks Coverage (exercises) 10:30 10:45 BREAK 10:45 11:30 Differential Accessibility 11:30 12:00 Differential Accessibility (exercises)  12:00 13:00 LUNCH BREAK 13:30 15:00 Peaks Annotation  15:00 15:15 BREAK 15:15 17:00 Peaks Annotation (exercises)"},{"location":"exercises/","title":"Exercises","text":""},{"location":"exercises/#material","title":"Material","text":"<p> Download the presentation</p> <ul> <li>Mkdocs Website</li> <li>Mkdocs material website</li> <li>Course website template on github</li> </ul>"},{"location":"exercises/#forking-and-cloning-the-template","title":"Forking and cloning the template","text":"<p>Go to https://github.com/sib-swiss/course_website_template, and click on Use this template:</p> <p>Choose the namespace in which you want to use the website template, choose a name, and initiate the new repository by finalising with Create repository from template:</p> <p>Now you can find the new repository at <code>https://github.com/[NAMESPACE]/[REPONAME]</code>. In order to clone the repository to a local directory, click on Code and copy the github address that you can use for cloning to your clipboard:</p> <p>After that, you open a terminal (e.g. Windows Powershell or your favourite terminal) <code>cd</code> to a directory you want to clone your repository in (e.g. to <code>C:\\Users\\myname\\Documents</code>) and type:</p> <pre><code>git clone https://github.com/[NAMESPACE]/[REPONAME].git # the last part can be pasted from github\n</code></pre>"},{"location":"exercises/#serving-a-website-locally","title":"Serving a website locally","text":"<p>In order to work on your website, it is convenient if you can serve it locally and directly see the effects of your work. In order to do that use the terminal to go into the repository directory, (e.g. <code>C:\\Users\\myname\\Documents\\reponame</code>) and type:</p> <pre><code>mkdocs serve\n</code></pre> <p>Now type <code>http://localhost:8000</code> in your favourite browser, and your website should be visible.</p> <p>Note</p> <p>The website already contains content. Of course, it is up to you whether you want to keep it. In any way, you can use it as an example on how to use markdown. </p> <p>Open the file <code>index.md</code> from the directory <code>docs</code> in your favourite text editor. Add some text to the page (e.g. <code>hello world!</code>) and save the file. See whether your changes are passed to the locally served website.</p> <p>Stopping <code>mkdocs serve</code></p> <p>After you have finished working on your website you will have to stop the serving process. Otherwise, it will continue in the background and keep port 8000 (and CPU) occupied. Stop the serving process with Ctrl+C .</p>"},{"location":"exercises/#the-file-structure","title":"The file structure","text":"<p>The total file structure of the template looks like this:</p> <pre><code>.\n\u251c\u2500\u2500 LICENCE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 assets\n\u2502   \u2502   \u2514\u2500\u2500 images\n\u2502   \u2502       \u251c\u2500\u2500 SIB_logo.svg\n\u2502   \u2502       \u251c\u2500\u2500 reactions_zoom.png\n\u2502   \u2502       \u251c\u2500\u2500 reply_in_thread.png\n\u2502   \u2502       \u2514\u2500\u2500 zoom_icons.png\n\u2502   \u251c\u2500\u2500 course_schedule.md\n\u2502   \u251c\u2500\u2500 exercises.md\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 precourse.md\n\u2502   \u2514\u2500\u2500 stylesheets\n\u2502       \u2514\u2500\u2500 extra.css\n\u2514\u2500\u2500 mkdocs.yml\n\n4 directories, 12 files\n</code></pre> <p>The main directory contains:</p> <ul> <li><code>LICENCE</code>: a licence file (in this case cc-by-4.0)</li> <li><code>README.md</code>: the readme displayed at the github repository</li> <li><code>docs</code>: a directory with all website content, including:<ul> <li><code>assets</code>: a directory everything that is not directly rendered (e.g. images, pdfs)</li> <li>files ending with <code>*.md</code>: the actual markdown files that are rendered into html</li> <li><code>stylesheets</code>: a directory with <code>.css</code> file(s) for defining the website style format</li> </ul> </li> <li><code>mkdocs.yml</code>: a YAML file that is used by <code>mkdocs</code> in which you specify:<ul> <li>Website structure</li> <li>Meta information</li> <li>Plugins</li> </ul> </li> </ul>"},{"location":"exercises/#setting-up-the-website-infrastructure","title":"Setting up the website infrastructure","text":"<p>Open <code>mkdocs.yml</code> in your favourite text editor. Have a look at the first part:</p> <pre><code>site_name: Course template\n\nnav:\n    - Home: index.md\n    - Precourse preparations: precourse.md\n    - Course schedule: course_schedule.md\n    - Exercises: exercises.md\n</code></pre> <p>The first line (<code>site_name</code>) let\u2019s you change website name. Change it to something that makes sense to you, and check whether it has changed in the locally hosted site.</p> <p>With the part named <code>nav</code>, you can change the website structure and with that navigation. The file <code>index.md</code> should always be there, this is the \u2018homepage\u2019.</p> <p>Now we will generate a new page that is a subchapter of Exercises. In order to do so, follow the following steps:</p> <ul> <li>Generate a directory within the directory <code>docs</code> called <code>exercises</code></li> <li>Within the <code>exercises</code> directory generate a new file called <code>exercises_day1.md</code></li> <li>Adjust the <code>nav</code> part of <code>mkdocs.yml</code> like so:</li> </ul> <pre><code>nav:\n    - Home: index.md\n    - Precourse preparations: precourse.md\n    - Course schedule: course_schedule.md\n    - Exercises:\n      - Day 1: exercises/exercises_day1.md\n</code></pre> <p>Now a new collapsible menu will appear, containing your new page.</p>"},{"location":"exercises/#referring-to-the-right-repo","title":"Referring to the right repo","text":"<p>In <code>mkdocs.yml</code> have a look at the repository part:</p> <pre><code># Repository\nrepo_name: sib-swiss/course_website_template\nrepo_url: https://github.com/sib-swiss/course_website_template\n</code></pre> <p>The course website is now hosted at your own repository. Therefore, change the repository name and url according to your own.</p>"},{"location":"exercises/#markdown-syntax","title":"Markdown syntax","text":"<p>You can use general github markdown syntax in order to generate a formatted html page. Have a look here.</p> <p>Now, convert the rendered text below into markdown. Add your markdown text to the file <code>exercises_day1.md</code> and see whether you get the expected result while you type.</p> Rendered markdown Answer <pre><code>### My markdown exercise\n\nWith plain markdown you can highlight in two ways:\n\n1. *Italic*\n2. **Bold**\n\nYou can add a link to your favourite [website](https://www.sib.swiss/).\nOr add an image from that website (find it at `https://www.sib.swiss/images/banners/banner_research_infrastructure.jpg`):\n\n![](https://www.sib.swiss/images/banners/banner_research_infrastructure.jpg)\n\nYou can also add a local image (this one is stored in `../assets/images/zoom_icons.png`):\n\n![](../assets/images/zoom_icons.png)\n\nSharing a code is easy, inline you refer to code like this: `pip install mkdocs`.\nBut often it's more convenient in a code block, e.g. with shell highlighting:\n\n```sh\nFILE=my_genes.csv\ncat $FILE | cut -f 1,2 -d ','\n```\n\nOr with R highlighting for example:\n\n```r\ndf &lt;- read.csv('my_genes.csv')\n```\n</code></pre>"},{"location":"exercises/#my-markdown-exercise","title":"My markdown exercise","text":"<p>With plain markdown you can highlight in two ways:</p> <ol> <li>Italic</li> <li>Bold</li> </ol> <p>You can add a link to your favourite website. Or add an image from that website (find it at <code>https://www.sib.swiss/images/banners/banner_research_infrastructure.jpg</code>):</p> <p></p> <p>You can also add a local image (this one is stored in <code>../assets/images/zoom_icons.png</code>):</p> <p></p> <p>Sharing a code is easy, inline you refer to code like this: <code>pip install mkdocs</code>. But often it\u2019s more convenient in a code block, e.g. with shell highlighting:</p> <pre><code>FILE=my_genes.csv\ncat $FILE | cut -f 1,2 -d ','\n</code></pre> <p>Or with R highlighting for example:</p> <pre><code>df &lt;- read.csv('my_genes.csv')\n</code></pre>"},{"location":"exercises/#additional-features-of-mkdocs-material","title":"Additional features of Mkdocs material","text":"<p>Some additional features are very convenient for generating a website for teaching. For example admonitions:</p> code <pre><code>!!! warning\n    Do not overcommit the server!\n</code></pre> output <p>Warning</p> <p>Do not overcommit the server!</p> <p>Also very convenient can be content tabs:</p> <p>code:</p> <pre><code>=== \"R\"\n    Generating a vector of integers:\n    ```r\n    a &lt;- c(5,4,3,2,1)\n    ```\n=== \"python\"\n    Generating a list of integers:\n    ```python\n    a = [5,4,3,2,1]\n    ```\n</code></pre> <p>output:</p> R <p>Generating a vector of integers: <pre><code>a &lt;- c(5,4,3,2,1)\n</code></pre></p> python <p>Generating a list of integers: <pre><code>a = [5,4,3,2,1]\n</code></pre></p> <p>Mkdocs material comes with a very wide range of emoticons and icons. Use the search field in the link to search for icons. Here\u2019s an example:</p> code <pre><code>Write an e-mail :material-send:, add a pdf :material-file-pdf: and wait :clock1:\n</code></pre> output <p>Write an e-mail , add a pdf :material-file-pdf: and wait </p> <p>You can make a button like this:</p> code <pre><code>[Download the presentation](../assets/pdf/introduction_gh_pages.pdf){: .md-button }\n</code></pre> output <p>Download the presentation</p> <p>You can also add an icon to a button:</p> code <pre><code>[:fontawesome-solid-file-pdf: Download the presentation](../assets/pdf/introduction_gh_pages.pdf){: .md-button }\n</code></pre> output <p> Download the presentation</p> <p>Lastly, you can incorporate <code>html</code>. This can particularly be convenient if you want to control the size of images.</p> code <pre><code>&lt;figure&gt;\n  &lt;img src=\"../assets/images/zoom_icons.png\" width=\"300\"/&gt;\n&lt;/figure&gt;\n\n&lt;figure&gt;\n  &lt;img src=\"../assets/images/zoom_icons.png\" width=\"100\"/&gt;\n&lt;/figure&gt;\n</code></pre> output <p> </p> <p> </p>"},{"location":"exercises/#byo-workshop","title":"BYO workshop","text":"<p>If you have brought your own course material, now you can start with generating a page containing your own course material.</p>"},{"location":"exercises/#host-the-website-at-githubio","title":"Host the website at github.io","text":"<p>You can deploy your website as a github page by running the command:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>It will become available at <code>[NAMESPACE].github.io/[REPONAME]</code>. This can take more than an hour if you are deploying for the first time. The next time you update your website, it will usually take less then a minute.</p>"},{"location":"exercises/#pushing-to-a-remote-repository","title":"Pushing to a remote repository","text":"<p>The website html is created in a directory called <code>site</code> inside your repository directory. This directory is used to locally host the website, but usually you don\u2019t want to push it to your master branch. Therefore add it to <code>.gitignore</code>:</p> <pre><code>echo \"site\" &gt;&gt; .gitignore\n</code></pre> <p>Note</p> <p>Anything that is added to the file <code>.gitignore</code> is not added to the git repository. You have to add such files/directories only once. You can of course also open <code>.gitignore</code> in your favourite text editor and modify it in there.</p> <p>Now, you can add your changes to make a commit:</p> <pre><code>git add --all\n</code></pre> <p>And commit your changes to your local repository like this:</p> <pre><code>git commit -m 'short description'\n</code></pre> <p>And finally push it to the remote:</p> <pre><code>git push\n</code></pre>"},{"location":"precourse/","title":"Precourse preparations","text":""},{"location":"precourse/#knowladge","title":"Knowladge","text":""},{"location":"precourse/#ngs","title":"NGS","text":"<p>As announced in the course registration webpage, we expect participants to already have a basic knowledge in Next Generation Sequencing (NGS) techniques.</p>"},{"location":"precourse/#unix","title":"UNIX","text":"<p>Practical knowledge of the UNIX command line is also required to be able to follow this course.</p>"},{"location":"precourse/#r","title":"R","text":"<p>A basic knowledge of the R language is required to perform most analytical stepsn the downstream analysis.</p>"},{"location":"precourse/#technical","title":"Technical","text":"<p>Attendees should have a Wi-Fi enabled computer and the Integrative Genomics Viewer (IGV) installed.</p> <p>An online R and RStudio environment will be provided. In order to access that environment your computer needs to be able to access http websites (not https). You can check this by browsing this website.</p>"},{"location":"days/00_connect_to_server/","title":"Connect to RStudio server","text":"<p>If you are enrolled in the course, you have access to RStudio server. You can find a link to it and your credentials to connect in the Google Docs that has been shared with you.  </p> <p>During the first part of the course we will be using bash commands and executing different software. For that, we will work on the built-in terminal of RStudio.   </p> <p>To access the terminal, select the tab Terminal on the top left of the panel:</p> <p></p> <p>Check your current directory, you should be in your home directory <code>/home/rstudio</code> </p> <pre><code>pwd\n</code></pre> <p>In order to activate your conda enviroment and have access to different tools necessary for this course, please run the following command in your terminal:</p> <pre><code>conda init\nsource /home/rstudio/.bashrc\nconda activate atac_env\n</code></pre> <p>You should now be able to run tools like samtools, bedtools, macs2, etc. Check with: <pre><code>bedtools --version\n</code></pre></p> <p>For the second part of the course we will use only R commands. For that, we will move back to RStudio console, by selecting the tab Console.</p>"},{"location":"days/01_filter_bam_files/","title":"01 Filter bam files","text":""},{"location":"days/01_filter_bam_files/#0-dataset","title":"0. Dataset","text":"<p>We are going to work with a subset of the publicly available dataset from Liu et al. 2019.</p> <p>We are going to analyse and compare ATAC-seq data from 2 different adult mouse tissues:     Kidney: Rep1, Rep2 Cerebrum: Rep1, Rep2 </p> <p>Raw data can be found in NCBI Sequence Read Archive</p> <p>In order to save time, raw reads have already been trimmed using Trim Galore and mapped to the reference genome (mm10) using bowtie2 in end-to-end mode. Therefore, we will start the analysis directly from the alignment output (.bam files).  </p> <p>To avoid large waiting times during high-demanding computational steps, .bam files have been subset to keep only reads aligning to chromosome 6.  </p> <p>Note</p> <p>You can find the .bam files in <code>/data/Liu_alignments_chr6/</code> folder</p>"},{"location":"days/01_filter_bam_files/#1-filtering-reads-from-alignments","title":"1. Filtering reads from alignments","text":""},{"location":"days/01_filter_bam_files/#general-ngs-filtering-steps","title":"General NGS filtering steps","text":"<p>In order to keep good quality data, read filtering criteria can be applied. These creteria are similar to the ones we would apply when analysing any other NGS related dataset (ie. remove duplicated reads, read mapping quality thresholds, etc.). They will depend on the quality of the data and the downstream analysis that will be applied.</p> <p>We can use tools like picard to indetify and mark read duplicates, which originate most likely from PCR amplifications. This step has already been performed for you, and read duplicates have been marked in the provided .bam files. If you want to know more about this tool and how it works you can have a look here.</p> <p>Task 1: </p> <ul> <li> <p>Create new directory for filtered bams: <code>results/01_filtered_bams</code></p> </li> <li> <p>Using <code>samtools view</code>, select/filter reads from .bam files in order to apply the following criteria: </p> <ul> <li>Keep only paired reads </li> <li>Remove unmapped reads</li> <li>Remove reads who\u2019s mate is unmapped</li> <li>Remove read duplicates</li> <li>Remove not primary alignment reads</li> <li>Keep mapping quality &gt; 10</li> </ul> </li> </ul> <p>Save the results in <code>results/01_filtered_bams</code> in BAM format with the following output name: <code>&lt;sample name&gt;.qc_filt.bam</code></p> <p>Samtools</p> <p>Use the samtools view manual here to understand which parameters you need, and this useful page from the broad institue to find what SAM Flag value would correspond to the combination of criteria we want to apply. </p> Hint You can use:    `-f` and `-F` flags to filter reads based on a combination of SAM Flags    `-q` for MapQ    `-h` to keep the .bam header    `-b` to get output in .bam format    Solution <pre><code># create new directory for filtered bams\n\nmkdir -p results/01_filtered_bams\n\n# filter files using a for loop\n\nfor bam in /data/Liu_alignments_chr6/*.bam; do\n    echo \"Processing file: $bam\"\n    sample_name=$(basename \"$bam\" .bam) # extract sample name without path and extension\n    samtools view -h -b -q 10 -f 1 -F 1292 -o results/01_filtered_bams/$sample_name.qc_filt.bam $bam\ndone\n</code></pre> <p>-h: keep header; -b: output in bam format; -q 10: minimum mapping quality 10; -f 1: keep paired; -F 1292: exclude reads with any of the following flags: read unmapped, mate unmapped, not primary alignment, read is duplicate</p> <p>After filtering we will sort and index the bam files for the next step</p> <p>Task 2: </p> <ul> <li>Use <code>samtools sort</code> to sort the filtered bam files (from previous task: <code>&lt;sample name&gt;.qc_filt.bam</code>). Save them in the same folder as: <code>&lt;sample name&gt;.qc_filt.sorted.bam</code></li> <li>Use <code>samtools index</code> to index the sorted bam files </li> </ul> Solution <pre><code>for bam in results/01_filtered_bams/*qc_filt.bam; do\n    echo \"Processing file: $bam\"\n    samtools sort -o ${bam%.bam}.sorted.bam $bam\n    samtools index ${bam%.bam}.sorted.bam\ndone\n</code></pre> <p>To avoid confusion and large size files, we will keep only the sorted and indexed bams: <pre><code>rm results/01_filtered_bams/*qc_filt.bam\n</code></pre></p>"},{"location":"days/01_filter_bam_files/#atacseq-related-filtering-steps","title":"ATACseq related filtering steps","text":"<p>Mitochondria DNA is nucleosome free, therefore it is more accessible for Tn5 and several reads may have originated from mitochondrial DNA. After having assessed mitochondrial % on the QC, we can discard reads coming from chmMT to avoid biases in downstream analsis.</p> <p>Since we are working only with chm 13 we don\u2019t need to do this step, but here is the command you could use for that: <pre><code>samtools view -h input.bam | awk  '($3 != \"MT\")' | samtools view -hb - &gt; output.bam\n</code></pre></p> <p>Note</p> <p>The mitochondrial chromosome name may differ depending on the reference genome (e.g., \u201cMT\u201d, \u201cchrM\u201d, \u201cchrMT\u201d).</p> <p>Next, we will remove reads overlapping problematic regions of the genome. ENCODE consortium has created comprehensive lists of such regions (anomalous, unstructured or high signal in NGS experiments) for different genome species (including mouse mm10). These lists are called ENCODE Blacklists, and you can find them here. </p> <p>Note</p> <p>The regions for mm10 have been dowloaded as .bed file, and you will find it here: <code>/data/references/mm10-blacklist.v2.nochr.bed</code></p> <p>Task 3</p> <ul> <li>Using <code>bedtools intersect</code> filter out reads overlapping regions in the Blacklist .bed file</li> <li>Use previously filtered .bam files as input: <code>results/01_filtered_bams/*qc_filt.sorted.bam</code>, don\u2019t forget to specify your input is in BAM format</li> <li>Use the <code>data/references/mm10-blacklist.v2.nochr.bed</code> as regions to filter out reads from (it is already sorted)</li> <li>Save the results in <code>results/01_filtered_bams</code> in BAM format with the following output name: <code>&lt;sample name&gt;.qc_bl_filt.bam</code> </li> <li>Sort and index the resulting .bam with <code>samtools</code> and save them in <code>results/01_filtered_bams</code> with name  <code>&lt;sample name&gt;.qc_bl_filt.sorted.bam</code> </li> </ul> <p>Bedtools</p> <p>Bedtools intersect allows one to screen for overlaps between two sets of genomic features/regions, and then decide on which kind of information do you want to report. Here, we will intersect: a) aligned reads to the genome (information in your .bam files)l b) Problematic regions listed in Blacklist .bed file We do not want to keep the reads that overlap Blacklist regions. You can find documentation on which parameters to use here</p> Solution <pre><code>blacklist=\"/data/references/mm10-blacklist.v2.nochr.bed\"\n\nfor bam in results/01_filtered_bams/*qc_filt.sorted.bam; do\n    echo \"Processing file: $bam\"\n    sample_name=$(basename \"$bam\" .qc_filt.sorted.bam) \n    bedtools intersect -v -abam $bam -b $blacklist &gt; results/01_filtered_bams/$sample_name.qc_bl_filt.bam\n    samtools sort -o results/01_filtered_bams/$sample_name.qc_bl_filt.sorted.bam results/01_filtered_bams/$sample_name.qc_bl_filt.bam\n    samtools index results/01_filtered_bams/$sample_name.qc_bl_filt.sorted.bam\ndone\n</code></pre> <p>Parameter explanation:  -v: only report those entries in A that have no overlap with B  -abam: input is in bam format </p> <p>After sorting the .bam files, we don\u2019t need the unsorted .bam. To free some space we will remove the unsorted bam files (intermediate files).</p> <p>Task 4</p> <ul> <li>Remove intermediate files that we don\u2019t need anymore. </li> </ul> <p>Warning</p> <p>Run this script only if you named the files in the same way the tutorial mentioned, and only if you have finished the sorting and indexing steps from Task 2 and 3</p> <pre><code>    rm results/01_filtered_bams/*qc_filt.bam\n    rm results/01_filtered_bams/*qc_bl_filt.bam\n</code></pre>"},{"location":"days/02_QC_post_alignment/","title":"02 QC post alignment","text":"<p>In order to assess the quality of the ATACseq experiment, there are several metrics that can be calculated from the aligned reads.</p> <p>Important metrics include:  - Fragment size distribution: The distribution of fragment sizes can indicate the quality of the library preparation. A good ATAC-seq library should have a characteristic pattern with peaks corresponding to nucleosome-free regions (NFRs) and mono-, di-, and tri-nucleosomes.  - TSS enrichment: The enrichment of reads at transcription start sites (TSS) is a key indicator of data quality. High-quality ATAC-seq data should show a strong enrichment of reads at TSSs.</p> <p><code>ATAQV</code> is a tool that calculates a variety of QC metrics for ATAC-seq data. It provides a comprehensive report that includes fragment size distribution, TSS enrichment, and other important metrics. </p> <p>Task 1</p> <ul> <li>Create a new folder for QC results called: <code>results/02_QC_post_aligment</code></li> <li>Run ATAQV on each filtered .bam file </li> <li>Using ATAQV tool, create a report from all ATAQV sample results</li> </ul> <p>Ataqv</p> <p>You can find further information about ATAQC tool and commands in the Usage section here Ataqv needs a Transcription Start Site (TSS) reference file to compute TSS enrichment score. You will find this file in: <code>data/references/ENCODE_mm10_M21_TSS_reference.bed</code> We have downloaded the TSS reference gile from ENCODE project (https://www.encodeproject.org/files/ENCFF498BEJ/)</p> Hint    First run ataqv for each \"*qc_bl_filt.sorted.bam\" file, important parameters are: --tss-file, mouse, --metrics-file    Second run mkarv with all \"*json\" files  Solution <p><pre><code>mkdir -p results/02_QC_post_aligment\nTSS_bed=\"/data/references/ENCODE_mm10_M21_TSS_reference.bed\"\n\nfor bam in results/01_filtered_bams/*qc_bl_filt.sorted.bam; do\n    echo \"Processing file: $bam\"\n    sample_name=$(basename \"$bam\" .qc_bl_filt.sorted.bam) # extract sample name without path and extension\n    echo $sample_name\n    ataqv --name $sample_name --metrics-file results/02_QC_post_aligment/$sample_name.ataqv.json --tss-file $TSS_bed mouse $bam &gt; results/02_QC_post_aligment/$sample_name.ataqv.out\ndone\n\ncd results/02_QC_post_aligment\nmkarv summary_ataqv Kidney_rep1.ataqv.json Kidney_rep2.ataqv.json Cerebrum_rep1.ataqv.json Cerebrum_rep2.ataqv.json \ncd ../../\n</code></pre> parametr explanation:  \u2013name: sample name for output files  \u2013metrics-file: output file for metrics in json format  \u2013tss-file: bed file with TSS locations  mouse: genome (can be mouse or human)  $bam: input bam file mkarv is a tool to create a summary report from multiple ataqv json files</p> <p>Task 2</p> <p>Open the QC report <code>results/02_QC_post_aligment/summary_ataqv/index.html</code> and have a look at the QC metrics. Do you think the experiment worked well?  Is there some metrics that would concern you?</p>"},{"location":"days/03_peak_calling/","title":"03 Peak calling","text":""},{"location":"days/03_peak_calling/#1-subset-bams-to-keep-only-nf-fragments","title":"1. Subset .bams to keep only NF fragments","text":"<p>Before doing peak calling with MACS3, we will split fragments into nucleosome-free (NF) and nucleosome-associated fragments.  For that, we will filter the BAM files based on fragment length (insert size). We will then call the peaks based on NF reads only, and compare with peaks called on all reads (?) Finally, we will also try peak calling with HMMRATAC, which is a peak caller specifically designed for ATAC-seq data and uses a Hidden Markov Model to identify accessible chromatin regions. (?)</p> <p>Task 1</p> <ul> <li>Create an output folder named: <code>results/01_filtered_bams/NF_bams</code></li> <li>Using <code>samtool view</code> keep only fragments with length: 1 - 100 bp. Keep output format as .bam file, and save them in <code>results/01_filtered_bams/NF_bams</code>. Follow the naming: ${sample_name}_NF.bam</li> <li>While doing that, keep the header of .bam file </li> </ul> <p>Note</p> <p>In SAM format, column 9 is described as \u201cTLEN: signed observed Template LENgth\u201d, which corresponds to the insert size </p> Hint You can first use samtools view, and then use awk commands to filter the reads for those which column 9 is: ($9&gt;= 1 &amp;&amp; $9&lt;=100) || ($9&lt;=-1 &amp;&amp; $9&gt;=-100)  Solution <p><pre><code># create new directory for peaks\nmkdir -p results/01_filtered_bams/NF_bams\n\n# filter for NF fragments only\n\nfor bam in results/01_filtered_bams/*qc_bl_filt.sorted.bam; do\n    sample_name=$(basename \"$bam\" .qc_bl_filt.sorted.bam)\n    echo \"Processing sample: $sample_name\"\n    samtools view -h $bam | awk 'substr($0,1,1)==\"@\" || ($9&gt;= 1 &amp;&amp; $9&lt;=100) || ($9&lt;=-1 &amp;&amp; $9&gt;=-100)' | \\\n    samtools view -b &gt; results/01_filtered_bams/NF_bams/${sample_name}_NF.bam \ndone \n</code></pre> Parameter explanation:  -h: keep header; -b: output in bam format; $9: insert size (TLEN field in SAM format); keep fragments with insert size between 1 and 100bp</p> <p>Task 2</p> <ul> <li>Sort and index the bams files for next step</li> <li>Remove the unsorted file</li> </ul> Solution <pre><code>for bam in results/01_filtered_bams/NF_bams/*_NF.bam; do\n    sample_name=$(basename \"$bam\" _NF.bam)\n    samtools sort -o results/01_filtered_bams/NF_bams/${sample_name}_NF.sorted.bam results/01_filtered_bams/NF_bams/${sample_name}_NF.bam\n    rm results/01_filtered_bams/NF_bams/${sample_name}_NF.bam\n    samtools index results/01_filtered_bams/NF_bams/${sample_name}_NF.sorted.bam\ndone\n</code></pre>"},{"location":"days/03_peak_calling/#2-peak-calling","title":"2. Peak calling","text":""},{"location":"days/03_peak_calling/#macs3-based-on-nf-fragments","title":"MACS3 based on NF fragments","text":"<p>Next we will call peaks using MACS3 on the NF reads only. This way, we focus on the fragments that are most likely to represent open chromatin regions only (nucleosome-free regions).</p> <p>MACS3 is a widely used peak calling tool that can handle both narrow and broad peaks. We will start using the \u201ccallpeak\u201d function of MACS3 to identify peaks in the NF reads.</p> <p>Task 3</p> <ul> <li>Create a new folder named: <code>results/03_peak_calling</code></li> <li>Create a subfolder inside called: <code>NF_peaks</code></li> <li>Call peaks on NF reads using MACS3 callpeak function. Save the results inside r<code>esults/03_peak_calling/NF_peaks/</code> and name the files as: <code>NF_peaks_${sample_name}</code></li> </ul> <p>Macs3</p> <p>You can have a look at the MACS3 documentation for more details on the parameters used here</p> Hint     For ATAC-seq data, we will use the BAMPE format, which is suitable for paired-end data and we will set the genome size to \"mm\" for mouse. We will also set a q-value cutoff of 0.01 to control the false discovery rate.  Solution <p><pre><code>mkdir -p results/03_peak_calling/NF_peaks\n\nsamples=(PC_rep1 PC_rep2 RACM_rep1 RACM_rep2)\nsamples=(Kidney_rep1 Kidney_rep2 Cerebrum_rep1 Cerebrum_rep2)\n\n# process NF reads\nbams_path=\"results/01_filtered_bams/NF_bams\"\n\nfor sample_name in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample\"\n    macs3 callpeak -f BAMPE -t $bams_path/${sample_name}_NF.sorted.bam -g mm -q 0.01 --outdir results/03_peak_calling/NF_peaks/NF_peaks_${sample_name}/\ndone    \n</code></pre> parameters explanation: -f BAMPE: input file format is BAM paired-end -t: input file (BAM file) -g mm: It\u2019s the mappable genome size or effective genome size (some are pre-computed, like mouse, and you can specify \u201cmm\u201d -q 0.01: q-value cutoff for peak detection \u2013outdir: output directory for peak files  </p>"},{"location":"days/03_peak_calling/#macs3-based-on-all-fragments","title":"MACS3 based on all fragments","text":"<p>We will do the same, but using all fragments in filtered .bam files </p> <p>Task 4</p> <ul> <li>Create a subfolder inside called: <code>all_peaks</code></li> <li>Call peaks on all filtered reads using MACS3 callpeak function. Save the results inside r<code>esults/03_peak_calling/all_peaks/</code> and name the files as: <code>all_peaks_${sample_name}</code></li> </ul> Solution <pre><code>bams_path=\"results/01_filtered_bams/\"\nmkdir -p results/03_peak_calling/all_peaks\n\nfor sample in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample\"\n    macs3 callpeak -f BAMPE -t $bams_path/${sample}.qc_bl_filt.sorted.bam -g mm -q 0.01 --outdir results/03_peak_calling/all_peaks/all_peaks_${sample}/\ndone\n</code></pre>"},{"location":"days/04_build_consensus_peak_set/","title":"04 Build consensus peak set","text":""},{"location":"days/04_build_consensus_peak_set/#1-visualisation-of-peaks","title":"1. Visualisation of peaks","text":"<p>We will start by having a look into the previous results. For that, we can use IGV</p>"},{"location":"days/04_build_consensus_peak_set/#2-buid-a-consensus-annotation","title":"2. Buid a consensus annotation","text":"<p>We need to build a consensus peak set for downstream analysis (e.g. differential accessibility analysis), which will become like our reference annotation for counting reads in peaks. We will do this by intersecting the peaks called in each replicate for each condition, and then merging the peaks from both conditions into a final consensus peak set.</p> <p>Run the following commands to create intersected peak files for each condition (PC and RACM), requiring at least 25% overlap (-f 0.25) and reciprocal overlap (-r)</p> <p>Code</p> <p>Create a folder for consensus peaks. Use bedtools intersect to keep peaks present in both replicates</p> <p><pre><code>    mkdir results/04_consensus_peaks\n    path_peaks=\"results/03_peak_calling/all_peaks\"\n\n    bedtools intersect -wa -a $path_peaks/all_peaks_Kidney_rep1/NA_peaks.narrowPeak -b $path_peaks/all_peaks_Kidney_rep2/NA_peaks.narrowPeak -f 0.25 -r &gt; results/04_consensus_peaks/Kidney_intersect.bed\n    bedtools intersect -wa -b $path_peaks/all_peaks_Kidney_rep1/NA_peaks.narrowPeak -a $path_peaks/all_peaks_Kidney_rep2/NA_peaks.narrowPeak -f 0.25 -r &gt;&gt; results/04_consensus_peaks/Kidney_intersect.bed\n    bedtools intersect -wa -a $path_peaks/all_peaks_Cerebrum_rep1/NA_peaks.narrowPeak -b $path_peaks/all_peaks_Cerebrum_rep2/NA_peaks.narrowPeak -f 0.25 -r &gt; results/04_consensus_peaks/Cerebrum_intersect.bed\n    bedtools intersect -wa -b $path_peaks/all_peaks_Cerebrum_rep1/NA_peaks.narrowPeak -a $path_peaks/all_peaks_Cerebrum_rep2/NA_peaks.narrowPeak -f 0.25 -r &gt;&gt; results/04_consensus_peaks/Cerebrum_intersect.bed\n</code></pre> Parameters explanation: -a: first input file (bed or narrowPeak format) -b: second input file (bed or narrowPeak format) -f 0.25: minimum overlap required as a fraction of A -r: require reciprocal overlap -wa: write the original entry in A for each overlap  </p> <p>To get all the columns from both files, we need to run bedtools intersect twice, swapping -a and -b</p> <p>Code</p> <p>Merge the intersected peaks from both conditions into a final consensus peak set, allowing a maximum distance of 10bp between peaks to be merged (-d 10)</p> <p><code>{bash} cat results/04_consensus_peaks/Kidney_intersect.bed results/04_consensus_peaks/Cerebrum_intersect.bed | sort -k1,1 -k2,2n | bedtools merge -d 10 -i - &gt; results/04_consensus_peaks/consensus_peaks.bed</code></p> <p>Add the <code>results/04_consensus_peaks/consensus_peaks.bed</code> track to IGV and have a look to the resulting peak annotation. </p>"},{"location":"days/05_quantify_peaks/","title":"05 Quantify peaks","text":""},{"location":"days/05_quantify_peaks/#1-count-reads-on-peaks","title":"1. Count reads on peaks","text":"<p>After defining a consensus peak set, we will count the number of reads that overlap each peak in each sample. This step can be seen as counting reads in annotated genes for RNA-seq data, where instead of genes, we are counting reads in peaks.</p> <p>There are different tools that can be used for this purpose, such as featureCounts from the Subread package or bedtools coverage. In this example, we will use featureCounts, which is a widely used tool for counting reads in genomic features. </p> <p>Task 1</p> <p>We first need to convert the consensus peaks to SAF format, which is the format required by featureCounts The SAF format is a tab-delimited file with the following columns: GeneID, Chr, Start, End, Strand</p> <ul> <li>Create a new folder named: <code>results/05_counts_reads</code></li> <li>Use awk or an alternative way to convert the bed file to SAF format. Don\u2019t forget to add a header \u201cGeneID    Chr Start   End Strand\u201d. </li> </ul> Solution <pre><code>mkdir results/05_counts_reads\necho \"GeneID    Chr Start   End Strand\" &gt; results/04_consensus_peaks/consensus_peaks.saf\nawk '{OFS = \"\\t\"} {print \"Interval_\"NR,$1,$2,$3,\".\"}' results/04_consensus_peaks/consensus_peaks.bed &gt;&gt; results/04_consensus_peaks/consensus_peaks.saf\n</code></pre> <p>Now we can use featureCounts to count the reads in each peak for each sample</p> <p>Task 2</p> <ul> <li>Run featureCounts to count filtered reads (from \u201c*qc_bl_filt.sorted.bam\u201d) into consensus peak annotation</li> </ul> <p>Featurecounts</p> <p>-F SAF: specify that the annotation file is in SAF format -p: specify that the input files are paired-end -a: specify the annotation file -o: specify the output file  </p> <p>These are only some parameteres that we will apply. You can also adjust the parameters of featureCounts based on your specific requirements, such as setting a minimum mapping quality or handling multi-mapping reads.</p> Solution <pre><code>path_bams=\"results/01_filtered_bams\"\nfeatureCounts -F SAF -T 2 -p -a results/04_consensus_peaks/consensus_peaks.saf -o results/05_counts_reads/feature_Counts.txt $path_bams/*qc_bl_filt.sorted.bam 2&gt; results/05_counts_reads/featureCounts.log\n</code></pre> <p>The output file will contain the counts of reads in each peak for each sample, which can be used for downstream analysis such as differential accessibility analysis.</p>"},{"location":"days/05_quantify_peaks/#2-assess-results-and-overall-quality","title":"2. Assess results and overall quality","text":"<p>After filtering, peak calling and counting reads on peaks, we can run MultiQC to aggregate the QC metrics from different steps and generate a comprehensive report. This will help us to assess further he overall quality of the ATAC-seq data and identify any potential issues that may need to be addressed before proceeding with downstream analysis.  </p> <p>Task 3 </p> <ul> <li>Run multiqc on the entire <code>results</code> directory</li> </ul> Solution <pre><code>multiqc --outdir results/multiQC_report --title multiQC_post_read_counting results\n</code></pre>"},{"location":"days/06_differential_accessibility_analysis/","title":"06 Differential accessibility analysis","text":"<p>In the next step, we will start importing the read counts on consensus peaks (output from featureCounts) into R and perform differential accessibility analysis using DESeq2.</p>"},{"location":"days/06_differential_accessibility_analysis/#1-import-read-counts-and-metadata","title":"1. Import read counts and metadata","text":"<p>We start by loading featureCounts output as a matrix of counts</p> <p>Load necessary packages for the analysis     <pre><code>library(DESeq2)\nlibrary(ggplot2)\nlibrary(dplyr)\n</code></pre></p> <p>Task 1</p> <ul> <li>Read the output of featurecounts that contains counts for all peaks in all samples: <code>results/05_counts_reads/feature_Counts.txt</code></li> <li>Keep the header of the table</li> <li>Process the table in order to keep: read-counts columns only, peak IDs as row.names, simple column names (sample names)</li> <li>Convert the table into matrix format</li> </ul> Solution <pre><code># Read table of counts (output from FeatureCounts)\ncounts_file &lt;- \"results/05_counts_reads/feature_Counts.txt\"\ncts_table &lt;- read.table(counts_file, header = T)\n\n# convert into matrix, with interval IDs as rownames\ncts &lt;- cts_table[,7:10]\nrow.names(cts) &lt;- cts_table[,1]\ncolnames(cts) &lt;- gsub(colnames(cts), pattern = \"results.01_filtered_bams.\", replacement = \"\") %&gt;% \n                 gsub(., pattern=\".qc_bl_filt.sorted.bam\", replacement=\"\")\n</code></pre> <p>Task 2</p> <p>Create metadata table We need to create a data.frame containing metadata information about the samples. Here we have two conditions, PC and RACM, with two replicates each. We need to create a data.frame containing the conditions information for each sample.  If we would have a second factor, e.g. treatment (treated vs untreated), we would need to add this information as a second column in the data.frame.</p> <p>Warning</p> <p>Important: The order of the rows in the data.frame must match the order of the columns in the counts matrix            The metadata data.frame must have factor variables (not character variables)</p> <pre><code>condition &lt;- factor( c(rep(\"Cerebrum\",2), rep(\"Kidney\",2)) )\ncolData &lt;- data.frame(condition, row.names = colnames(cts))\n</code></pre>"},{"location":"days/06_differential_accessibility_analysis/#2-deseq-object-and-analysis","title":"2. DEseq object and analysis","text":"<p>Bring together counts and metadata to create a DESeq object</p> <pre><code>dds &lt;- DESeqDataSetFromMatrix(\n  countData = cts, colData = colData, \n  design = ~ condition)\ndim(dds)\n</code></pre> <p>Remove lowly exp peaks</p> <pre><code>idx &lt;- rowSums(counts(dds, normalized=FALSE) &gt;= 50) &gt;= 2\ndds.f &lt;- dds[idx, ]\ndim(dds.f)\n</code></pre> <p>We perform the estimation of dispersions</p> <pre><code>dds &lt;- DESeq(dds)\n</code></pre> <p>And plot PCA of the samples <pre><code>vsd &lt;- varianceStabilizingTransformation(dds, blind=TRUE )\npcaData &lt;- plotPCA(vsd, intgroup=c(\"condition\"), ntop=\"all\")\npcaData + geom_label(aes(x=PC1,y=PC2,label=name))\nplotPCA(vsd, intgroup=c(\"sizeFactor\"), ntop=\"all\")\n</code></pre></p>"},{"location":"days/06_differential_accessibility_analysis/#3-save-differential-accessibility-results","title":"3. Save differential Accessibility results","text":"<p>After the dispersion estimates have been calculated, we can proceed to test for differential accessibility between the two conditions (PC vs RACM).  </p> <p>The <code>results()</code> function extracts a results table with the log2 fold changes, p-values and adjusted p-values for each peak. <pre><code>res &lt;- results(dds)\nsummary( res )\n</code></pre></p> <p>We will save the results table in a new directory <pre><code>dir.create(\"results/06_DA_analysis\")\nDA_results &lt;- as.data.frame(res)\nwrite.table(DA_results, file=\"results/06_DA_analysis/DA_results.txt\", quote=FALSE)\n</code></pre></p> <p>Note</p> <p>By default, the results() function will extract the results for the last variable in the design formula (here: condition) and will perform a comparison of the second level of the factor over the first level (here: RACM over PC). If you want to extract results for a different comparison, you can specify the contrast argument. Have a look at DESeq2 documentation for more information.</p> <p>We can visualise the results of DA peaks with a Volcano plot, analogous to RNAseq DE genes results</p> <pre><code>\n</code></pre>"},{"location":"days/06_differential_accessibility_analysis/#create-granges-object","title":"Create GRanges object","text":"<p>We will convert the peak coordinates into a GRanges object (from GenomicRanges package).  Granges class represents a collection of genomic ranges and associated data to them. In this case, we will use it to represent the peak coordinates, and we will add as metadata the log2 fold changes and adjusted p-values from the differential accessibility analysis.</p> <p><pre><code>    # Prepare peak coordinates object \n    peaks_coord &lt;- cts_table[,1:4]\n\n    # Select information from DESeq2 we want to keep\n    DA_stats &lt;- as.data.frame(res)[c(2,3,5,6)]\n\n    # Merge both objects and convert them into GRanges class\n    peaks_df &lt;- merge(DA_stats, peaks_coord, by.x=\"row.names\", by.y=\"Geneid\")\n    peaks_gr = makeGRangesFromDataFrame(peaks_df, keep.extra.columns=T)\n\n    # Have a look into the new object\n    head(peaks_gr)\n</code></pre> To this object <code>peaks_gr</code>, we will add other metadata along the downstream analysis. </p> <pre><code>rm(list = ls())\n</code></pre>"},{"location":"days/07_annotate_peaks/","title":"Peak Annotation and Functional Analysis","text":"<p>After performing differential accessibility analysis with DESeq2, we will annotate peaks based on their genomic location using ChIPseeker and perform functional enrichment analysis.</p>"},{"location":"days/07_annotate_peaks/#overview","title":"Overview","text":"<p>In this section, we will: 1. Load the necessary libraries for peak annotation 2. Prepare our differential accessibility results for annotation 3. Visualize peak distribution across the genome 4. Annotate peaks with genomic features 5. Perform functional enrichment analysis</p>"},{"location":"days/07_annotate_peaks/#1-load-required-libraries","title":"1. Load Required Libraries","text":"<p>First, let\u2019s load the necessary R packages for peak annotation and functional analysis:</p> <pre><code># Load libraries\nlibrary(ChIPseeker)\nrequire(\"TxDb.Mmusculus.UCSC.mm10.knownGene\")\nlibrary(clusterProfiler)\nlibrary(\"org.Mm.eg.db\")\n\n# Set up annotation databases\nTxDb &lt;- TxDb.Mmusculus.UCSC.mm10.knownGene\nAnnoDb &lt;- 'org.Mm.eg.db'\n</code></pre>"},{"location":"days/07_annotate_peaks/#2-prepare-differential-accessibility-results","title":"2. Prepare Differential Accessibility Results","text":"<p>Now let\u2019s prepare our data by combining the differential accessibility statistics with the count data:</p> <pre><code># Load again the counts table\ncounts_file &lt;- \"results/05_counts_reads/feature_Counts.txt\"\ncts_table &lt;- read.table(counts_file, header = T)\n\n# Load the results from DESeq2 and extract relevant columns from DESeq2 results\nres &lt;- read.table(\"results/06_DA_analysis/DA_results.txt\")\nDA_stats &lt;- as.data.frame(res)[c(2,3,5,6)]\n\n# Clean column names from count table\ncolnames(cts_table) &lt;- gsub(colnames(cts_table), pattern = \"results.01_filtered_bams.\", replacement = \"\") %&gt;% \n  gsub(., pattern=\".qc_bl_filt.sorted.bam\", replacement=\"\")\n\n# Merge differential accessibility stats with count data\npeaks_df &lt;- merge(DA_stats, cts_table, by.x=\"row.names\", by.y=\"Geneid\")\n\n# Add 'chr' prefix to chromosome names for consistency\npeaks_df$Chr &lt;- paste0(\"chr\", peaks_df$Chr)\n\n# Create GRanges object for downstream analysis\ngr &lt;- makeGRangesFromDataFrame(peaks_df, keep.extra.columns=TRUE)\n</code></pre>"},{"location":"days/07_annotate_peaks/#3-visualize-peak-distribution","title":"3. Visualize Peak Distribution","text":"<p>Let\u2019s visualize how our peaks are distributed across the genome and their fold changes:</p> <pre><code># Create coverage plot showing peak positions and their fold changes\ncovplot(gr, weightCol = \"log2FoldChange\")\n</code></pre> <p>This plot shows the distribution of peaks across chromosomes, with y-axis representing the log2 fold change values.</p>"},{"location":"days/07_annotate_peaks/#4-prepare-data-for-annotation","title":"4. Prepare Data for Annotation","text":"<pre><code># Sort the GRanges object by genomic coordinates\ngr &lt;- sort(gr, by = ~ seqnames + start + end)\n\n# Split peaks into upregulated and downregulated based on significance and fold change\ngr_list &lt;- list(\n  up = gr[gr$padj &lt; 0.01 &amp; gr$log2FoldChange &gt; 2,], \n  down = gr[gr$padj &lt; 0.01 &amp; gr$log2FoldChange &lt; -2,]\n)\n\n\n\n\n# Check the structure of our peak lists\nhead(gr_list)\n</code></pre> <p>Task 1 </p> <ul> <li>Check how many significantly upregulated and downregulated peaks we have:</li> </ul> <pre><code># Count peaks in each category\ncat(\"Number of upregulated peaks:\", length(gr_list$up), \"\\n\")\ncat(\"Number of downregulated peaks:\", length(gr_list$down), \"\\n\")\n</code></pre>"},{"location":"days/07_annotate_peaks/#5-annotate-genomic-overlap-with-chipseeker","title":"5. Annotate Genomic Overlap with ChIPseeker","text":"<p>Now we\u2019ll annotate our peaks to determine their genomic context (promoters, exons, introns, etc.):</p> <pre><code># Annotate upregulated peaks\npeakAnno_up &lt;- annotatePeak(gr_list$up, \n                           tssRegion = c(-1000, 1000), \n                           TxDb = TxDb, \n                           annoDb = AnnoDb, \n                           overlap = \"TSS\")\n\n# Annotate downregulated peaks\npeakAnno_down &lt;- annotatePeak(gr_list$down, \n                             tssRegion = c(-1000, 1000), \n                             TxDb = TxDb, \n                             annoDb = AnnoDb, \n                             overlap = \"TSS\")\n\n\n# Save the objects\n# Save the objects\nsaveRDS(peakAnno_up, \"results/06_DA_analysis/Annotated_peaks_up.rds\")\nsaveRDS(peakAnno_down, \"results/06_DA_analysis/Annotated_peaks_down.rds\")\nsaveRDS(gr, \"results/06_DA_analysis/gr_peaks.rds\")\n</code></pre> <p>Task 2 </p> <ul> <li>Examine the annotation results</li> </ul> <pre><code># Look at the upregulated peaks annotation summary\npeakAnno_up\n\n# Count how many downregulated peaks are in promoter regions\nsum(peakAnno_down@detailGenomicAnnotation$Promoter)\n</code></pre>"},{"location":"days/07_annotate_peaks/#visualize-peak-annotations","title":"Visualize Peak Annotations","text":"<p>Create pie charts to visualize the genomic distribution of our peaks:</p> <pre><code># Plot pie charts for peak categories\nplotAnnoPie(peakAnno_up, main = \"\\n\\nDA up\") \nplotAnnoPie(peakAnno_down, main = \"\\n\\nDA down\")\n</code></pre> <p>These plots show the percentage of peaks falling into different genomic categories (promoters, exons, introns, intergenic regions, etc.).</p>"},{"location":"days/07_annotate_peaks/#6-functional-enrichment-analysis","title":"6. Functional Enrichment Analysis","text":"<p>Next, we will focus in peaks overlaping promoter regions (TSS) of genes, and we will perform functional enrichment analysis on those genes to understand which biological processes or metabolic pathways may be afected by the changes in chromatin accessibility.</p>"},{"location":"days/07_annotate_peaks/#extract-genes-associated-with-promoter-peaks","title":"Extract Genes Associated with Promoter Peaks","text":"<pre><code># Extract gene symbols for peaks in promoter regions\ngenes_tss_up &lt;- peakAnno_up@anno[peakAnno_up@detailGenomicAnnotation$Promoter, \"SYMBOL\"]\ngenes_tss_down &lt;- peakAnno_down@anno[peakAnno_down@detailGenomicAnnotation$Promoter, \"SYMBOL\"]\n\n# Create universe of genes with all promoter-overlapping peaks for background\npeakAnno &lt;- annotatePeak(gr, \n                        tssRegion = c(-1000, 1000), \n                        TxDb = TxDb, \n                        annoDb = AnnoDb, \n                        overlap = \"TSS\")\nuniverse &lt;- peakAnno@anno[peakAnno@detailGenomicAnnotation$Promoter,]$SYMBOL\n</code></pre>"},{"location":"days/07_annotate_peaks/#gene-ontology-go-enrichment-analysis","title":"Gene Ontology (GO) Enrichment Analysis","text":"<p>Perform GO enrichment analysis for upregulated peaks:</p> <pre><code># GO enrichment for upregulated genes\nego_up &lt;- enrichGO(gene          = genes_tss_up$SYMBOL,\n                   universe      = unique(universe),\n                   OrgDb         = org.Mm.eg.db,\n                   keyType       = \"SYMBOL\",\n                   ont           = \"BP\",  # Molecular Function\n                   pAdjustMethod = \"BH\",\n                   pvalueCutoff  = 0.05,\n                   qvalueCutoff  = 0.05,\n                   readable      = TRUE)\n\n# Plot results\nego_up\n</code></pre> <p>Task 3 </p> <ul> <li> <p>Did you find significantly enriched GO terms?  </p> </li> <li> <p>Try to run the same analysis considering all genes as a universe, why do you think there is a difference?  </p> </li> <li> <p>Visualise the GO terms using a function from ClusterProfiler such as: <code>barplot()</code></p> </li> </ul> Solution <pre><code># GO enrichment for upregulated genes\nego_up &lt;- enrichGO(gene          = genes_tss_up$SYMBOL,\n                universe      = unique(universe),\n                OrgDb         = org.Mm.eg.db,\n                keyType       = \"SYMBOL\",\n                ont           = \"BP\",  # Molecular Function\n                pAdjustMethod = \"BH\",\n                pvalueCutoff  = 0.05,\n                qvalueCutoff  = 0.05,\n                readable      = TRUE)\n\n# Plot results\nprint(barplot(ego_down, showCategory = 20))\n</code></pre> <p>Task 4</p> <ul> <li>Perform GO enrichment analysis for downregulated peaks using all genes as universe:</li> </ul> Solution <pre><code># GO enrichment for downregulated genes\nego_down &lt;- enrichGO(gene          = genes_tss_down$SYMBOL,\n                    OrgDb         = org.Mm.eg.db,\n                    keyType       = \"SYMBOL\",\n                    ont           = \"BP\",  # Molecular Function\n                    pAdjustMethod = \"BH\",\n                    pvalueCutoff  = 0.05,\n                    qvalueCutoff  = 0.05,\n                    readable      = TRUE)\n\n# Plot results\nprint(barplot(ego_down, showCategory = 20))\n</code></pre> <p>Next Steps</p> <p>You can change the <code>ont</code> parameter to explore different aspects: - \u201cMF\u201d: Molecular Function - \u201cBP\u201d: Biological Process - \u201cCC\u201d: Cellular Component</p>"},{"location":"days/07_annotate_peaks/#7-additional-visualizations","title":"7. Additional Visualizations","text":""},{"location":"days/07_annotate_peaks/#peak-heatmap","title":"Peak Heatmap","text":"<p>Create a heatmap showing peak signal around gene bodies:</p> <pre><code># Generate peak heatmap around gene bodies\npeakHeatmap(peak = gr,\n            TxDb = TxDb,\n            upstream = rel(0.2),\n            downstream = rel(0.2),\n            by = \"gene\",\n            type = \"body\",\n            nbin = 800)\n</code></pre>"},{"location":"days/07_annotate_peaks/#load-additional-reference-data","title":"Load Additional Reference Data","text":"<pre><code># Load TSS reference data for additional analysis\ntss_bed &lt;- read.table(\"data/references/ENCODE_mm10_M21_TSS_reference.bed\")\n</code></pre>"},{"location":"days/07_annotate_peaks/#summary","title":"Summary","text":"<p>In this tutorial, we have:</p> <ol> <li>\u2705 Loaded necessary libraries for peak annotation</li> <li>\u2705 Prepared differential accessibility results for analysis</li> <li>\u2705 Visualized peak distribution across the genome</li> <li>\u2705 Annotated peaks with genomic features using ChIPseeker</li> <li>\u2705 Performed functional enrichment analysis on genes associated with promoter peaks</li> <li>\u2705 Created visualizations to interpret the biological significance of our results</li> </ol> <p>The results from this analysis help us understand: - Where our differentially accessible peaks are located in the genome - Which genes might be affected by changes in chromatin accessibility - What biological processes or molecular functions are enriched in our peak sets</p> <p>Next Steps</p> <p>Consider exploring different GO ontology categories (BP, CC) and other enrichment databases like KEGG pathways for a more comprehensive functional analysis.</p>"}]}